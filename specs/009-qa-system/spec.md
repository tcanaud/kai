# Feature Specification: QA System

**Feature Branch**: `009-qa-system`
**Created**: 2026-02-18
**Status**: Draft
**Input**: User description: "QA System — governance-native verification that closes the kai governance loop by generating executable test scripts from specifications and agreements"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Generate Test Plan from Specifications (Priority: P1)

A developer has finished implementing a feature via `/speckit.implement` and all tasks are marked done. The developer needs mechanical proof that acceptance criteria from `spec.md` are satisfied before creating a PR. The developer runs `/qa.plan {feature}`.

The system reads the project's `.knowledge/` base to understand the development environment, testing conventions, and tech stack. It then reads `spec.md` acceptance criteria and `agreement.yaml` interfaces. It explores the codebase to understand existing patterns and entry points. Finally, it generates executable test scripts — one per acceptance criterion — stored in `.qa/{feature}/scripts/`, and produces a `_index.yaml` mapping each script to its corresponding acceptance criterion, including source file checksums for freshness tracking.

**Why this priority**: Without test plan generation, no other QA capability is possible. This is the foundation — the system's core value proposition.

**Independent Test**: Can be fully tested by running `/qa.plan` on a feature with a completed `spec.md` and verifying that executable scripts appear in `.qa/{feature}/scripts/` with a valid `_index.yaml`.

**Acceptance Scenarios**:

1. **Given** a feature with a completed `spec.md` containing 5 acceptance criteria, **When** the developer runs `/qa.plan {feature}`, **Then** at least 5 executable test scripts are generated in `.qa/{feature}/scripts/`, each mapped to a criterion in `_index.yaml`.
2. **Given** a project with `.knowledge/` guides describing Node.js ESM conventions, **When** `/qa.plan` generates scripts, **Then** the scripts use project-appropriate patterns (e.g., `node:test` for assertions, ESM imports) rather than generic boilerplate.
3. **Given** a feature with both `spec.md` and `agreement.yaml`, **When** `/qa.plan` runs, **Then** the system reads both artifacts and generates scripts covering both acceptance criteria and interface compliance.
4. **Given** a feature with `spec.md` but no `agreement.yaml`, **When** `/qa.plan` runs, **Then** the system proceeds using `spec.md` alone and generates scripts without error.
5. **Given** a project with no `.knowledge/` directory, **When** `/qa.plan` runs, **Then** the system still generates functional test scripts (less project-specific but executable).

---

### User Story 2 - Execute Tests and Get Verdict (Priority: P1)

A developer has a test plan generated by `/qa.plan` and wants to know whether the feature passes or fails. The developer runs `/qa.run {feature}`.

The system first checks that the test plan is fresh (source checksums match current files). If fresh, it executes each script independently, captures per-script PASS/FAIL results, and produces a binary aggregate verdict: PASS only if all scripts pass, FAIL if any script fails. For failures, the output identifies the exact script, assertion, and expected-vs-actual values. Non-blocking findings (drift, edge cases, improvements) are deposited in `.product/inbox/` as structured feedback files.

**Why this priority**: Equally critical as P1 — plan generation without execution delivers no verdict. These two stories together form the minimum viable loop.

**Independent Test**: Can be tested by running `/qa.run` on a feature with pre-generated test scripts and verifying the verdict output and finding deposits.

**Acceptance Scenarios**:

1. **Given** a feature with 10 test scripts where all pass, **When** the developer runs `/qa.run {feature}`, **Then** the verdict is PASS with 10/10 scripts passing.
2. **Given** a feature with 10 test scripts where 2 fail, **When** the developer runs `/qa.run {feature}`, **Then** the verdict is FAIL with per-script detail showing which scripts failed, which assertions broke, and expected vs actual values.
3. **Given** a test run that discovers a non-blocking finding (edge case, improvement), **When** the run completes, **Then** the finding is deposited in `.product/inbox/` as a structured feedback file including test script path, execution result, and link to the related acceptance criterion.
4. **Given** a stale test plan (source checksums do not match current files), **When** the developer runs `/qa.run {feature}`, **Then** the system refuses to execute and directs the developer to run `/qa.plan {feature}` to regenerate.
5. **Given** a project without a `.product/` directory, **When** `/qa.run` discovers non-blocking findings, **Then** the verdict is still produced but finding deposit is skipped with a warning.

---

### User Story 3 - Check Test Plan Freshness Across Features (Priority: P2)

A developer managing multiple features wants to know which test plans are up to date before a release or bulk QA pass. The developer runs `/qa.check`.

The system scans all `_index.yaml` files across `.qa/` directories, compares stored checksums against current `spec.md` and `agreement.yaml` files, and reports per-feature freshness status: current (checksums match) or stale (checksums differ, with the specific changed file identified).

**Why this priority**: Important for operational hygiene but not required for the core plan-run loop. A developer can manually track freshness without this command.

**Independent Test**: Can be tested by running `/qa.check` on a project with multiple features where some specs have changed since last `/qa.plan`.

**Acceptance Scenarios**:

1. **Given** 3 features with `.qa/` directories where all test plans are current, **When** the developer runs `/qa.check`, **Then** all 3 features show "current" status.
2. **Given** 3 features where 1 feature's `spec.md` has changed since last `/qa.plan`, **When** the developer runs `/qa.check`, **Then** that feature shows "stale" with identification of `spec.md` as the changed source file.
3. **Given** a feature with no `.qa/` directory, **When** `/qa.check` runs, **Then** that feature is either skipped or reported as "no test plan" — not an error.

---

### User Story 4 - Traceability Chain for Reviewers (Priority: P2)

A reviewer receives a PR and needs to understand what was verified, how, and with what results — without tribal knowledge. The reviewer reads `.qa/{feature}/_index.yaml` to see the script-to-criterion mapping, inspects individual test scripts to understand verification logic, and checks results for the last run verdict.

**Why this priority**: Important for team trust and auditability but secondary to the developer's own plan-run cycle.

**Independent Test**: Can be tested by verifying that `_index.yaml` contains readable mappings from script paths to acceptance criteria, and that scripts are self-documenting (comments, clear assertions).

**Acceptance Scenarios**:

1. **Given** a completed QA run, **When** a reviewer reads `_index.yaml`, **Then** they can see which acceptance criterion each script verifies, including the criterion text and spec section reference.
2. **Given** any test script in `.qa/{feature}/scripts/`, **When** a reviewer reads the script, **Then** they can understand what it verifies without needing external context (script contains comments linking to the criterion).
3. **Given** a non-blocking finding deposited in `.product/inbox/`, **When** a reviewer traces it, **Then** they can follow the chain: finding → test script → acceptance criterion → spec.md.

---

### User Story 5 - Package Installation and Updates (Priority: P3)

A developer wants to add the QA system to their kai project. They run `npx @tcanaud/qa-system install`, which copies slash command templates to `.claude/commands/`. Later, they run `npx @tcanaud/qa-system update` to get newer command templates.

**Why this priority**: Distribution mechanism — necessary for adoption but the commands themselves (P1/P2) deliver the value.

**Independent Test**: Can be tested by running the install command on a fresh project and verifying that command templates appear in `.claude/commands/`.

**Acceptance Scenarios**:

1. **Given** a kai project without the QA system installed, **When** the developer runs `npx @tcanaud/qa-system install`, **Then** slash command templates for `/qa.plan`, `/qa.run`, and `/qa.check` are copied to `.claude/commands/`.
2. **Given** a project with an older version of command templates, **When** the developer runs `npx @tcanaud/qa-system update`, **Then** the templates are replaced with the latest versions.
3. **Given** the installed package, **When** inspecting its dependencies, **Then** it has zero runtime dependencies — only `node:` protocol imports.

---

### Edge Cases

- What happens when `/qa.plan` is run on a feature with no `spec.md`? The system reports an error — a spec is required for test generation.
- What happens when a test script fails to execute (syntax error, missing dependency)? The script is marked FAIL with the execution error as the failure detail — it does not crash the entire run.
- What happens when `/qa.run` is run on a feature with no `.qa/` directory? The system reports that no test plan exists and directs the developer to run `/qa.plan` first.
- What happens when `_index.yaml` is manually edited or corrupted? The system validates `_index.yaml` structure before execution and reports errors if the format is invalid.
- What happens when a spec has criteria but the generated scripts cannot actually test them (e.g., criterion requires user interaction)? The script is generated with a clear comment indicating it requires manual verification, and it is excluded from the automated verdict.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST generate at least one executable test script per acceptance criterion in `spec.md` when `/qa.plan {feature}` is run
- **FR-002**: System MUST consult `.knowledge/` guides to discover the project's development environment, testing conventions, and tech stack before generating scripts
- **FR-003**: System MUST read `agreement.yaml` interfaces and generate scripts that verify interface compliance when an agreement exists
- **FR-004**: System MUST store generated scripts as executable files in `.qa/{feature}/scripts/`
- **FR-005**: System MUST produce a `_index.yaml` file mapping each script to the acceptance criterion it verifies, including source file checksums (SHA-256)
- **FR-006**: System MUST execute all scripts for a feature when `/qa.run {feature}` is run, capturing per-script PASS/FAIL results
- **FR-007**: System MUST produce a binary aggregate verdict: PASS if all scripts pass, FAIL if any script fails
- **FR-008**: System MUST report per-script failure details including script name, assertion description, and expected vs actual values
- **FR-009**: System MUST compare stored checksums against current source files before execution and refuse to run if the test plan is stale
- **FR-010**: System MUST deposit non-blocking findings in `.product/inbox/` as structured feedback files conforming to the product-manager feedback schema
- **FR-011**: Each deposited finding MUST include the test script path, execution result, and link to the related acceptance criterion
- **FR-012**: System MUST report per-feature freshness status (current/stale) when `/qa.check` is run, identifying which source file changed
- **FR-013**: System MUST proceed with test generation using `spec.md` alone when `agreement.yaml` does not exist
- **FR-014**: System MUST generate functional (less project-specific) test scripts when `.knowledge/` is empty or missing
- **FR-015**: System MUST still produce a verdict when `.product/` directory does not exist, skipping finding deposit with a warning
- **FR-016**: System MUST provide an installer (`npx @tcanaud/qa-system install`) that copies slash command templates to `.claude/commands/`
- **FR-017**: System MUST provide an updater (`npx @tcanaud/qa-system update`) that replaces command templates with the latest versions
- **FR-018**: System MUST operate with zero runtime dependencies — `node:` protocol imports only

### Key Entities

- **Test Plan**: The complete set of generated test scripts for a feature, tracked by `_index.yaml`. Contains script-to-criterion mappings and source checksums.
- **Test Script**: An individual executable file (shell, JS, or project-appropriate) that verifies one or more acceptance criteria. Stored in `.qa/{feature}/scripts/`.
- **Index (`_index.yaml`)**: Per-feature metadata file mapping scripts to criteria, storing source checksums, and recording generation timestamp.
- **Verdict**: The aggregate result of a `/qa.run` — binary PASS/FAIL with per-script detail.
- **Finding**: A non-blocking observation from a test run (drift, edge case, improvement) deposited as structured feedback in `.product/inbox/`.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Every acceptance criterion in `spec.md` has a corresponding executable test script after first `/qa.plan` — coverage target: 100%
- **SC-002**: When `/qa.run` reports PASS, fewer than 5% of features have regressions discovered post-merge
- **SC-003**: A developer completes the full cycle (`/qa.plan` → `/qa.run` → verdict) in a single session with no manual test setup
- **SC-004**: `/qa.check` correctly identifies 100% of stale test plans when source files have changed
- **SC-005**: Every non-blocking finding deposited in `.product/inbox/` includes script path, result, and criterion link — traceability target: 100%
- **SC-006**: Given identical test scripts and identical code, `/qa.run` produces the same verdict on every execution — reproducibility target: 100%
- **SC-007**: Test scripts are self-contained and executable by any developer with repository access, without environment-specific setup beyond what `.knowledge/` documents

## Assumptions

- The target project uses kai's governance pipeline (`.features/`, `specs/`, `.agreements/`). The QA system is not a general-purpose testing tool.
- `spec.md` contains structured acceptance scenarios in Given/When/Then format that can be parsed for test generation.
- The project's `.knowledge/` guides, when present, contain sufficient information about the tech stack and testing conventions to produce idiomatic scripts.
- SHA-256 checksums are sufficient for freshness detection. Content-aware diffing (ignoring whitespace/comments) is deferred to a future version.
- The product-manager feedback schema (`id`, `title`, `category`, `source`, `created`, `linked_to`) is stable and will not change during QA system development.
- Generated test scripts target the project's existing tooling — no new test framework installation is required.
